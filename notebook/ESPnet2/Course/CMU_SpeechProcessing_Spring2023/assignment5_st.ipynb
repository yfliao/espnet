{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saK2jMSW8s0T"
   },
   "source": [
    "# CMU 11492/11692 Spring 2023: Speech Translation\n",
    "\n",
    "In this demonstration, we will show you some demonstrations of speech translation systems in ESPnet. \n",
    "\n",
    "Main references:\n",
    "- [ESPnet repository](https://github.com/espnet/espnet)\n",
    "- [ESPnet documentation](https://espnet.github.io/espnet/)\n",
    "- [ESPnet-ST-v2 demo](https://colab.research.google.com/drive/1htcM-N6ChTB1tx3C69WqQ6JhyevqKskH?usp=sharing)\n",
    "- [ESPnet-ST repo (WIP)](https://github.com/ftshijt/espnet/tree/merge_s2st_st)\n",
    "\n",
    "Author:\n",
    "- Jiatong Shi (jiatongs@andrew.cmu.edu)\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After this demonstration, you are expected to understand some latest advancements in speech translation.\n",
    "\n",
    "## ❗Important Notes❗\n",
    "- We are using Colab to show the demo. However, Colab has some constraints on the total GPU runtime. If you use too much GPU time, you may not be able to use GPU for some time.\n",
    "- There are multiple in-class checkpoints ✅ throughout this tutorial. **Your participation points are based on these tasks.** Please try your best to follow all the steps! If you encounter issues, please notify the TAs as soon as possible so that we can make an adjustment for you.\n",
    "- Please submit PDF files of your completed notebooks to Gradescope. You can print the notebook using `File -> Print` in the menu bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awospJV5LT9m"
   },
   "source": [
    "## ESPnet installation (Inference vesion)\n",
    "\n",
    "Different from previous assignment where we install the full version of ESPnet, we use a lightweight ESPnet package, which mainly designed for inference purpose. The installation with the light version can be much faster than a full installation. Noted that this is an active on-going work in ESPnet. The codebase is still in merging, so we will use a branch from our development fork for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHz24kuzNGe8"
   },
   "outputs": [],
   "source": [
    "!pip install typeguard==2.13.3\n",
    "!git clone --depth 5 -b merge_s2st_st https://github.com/ftshijt/espnet.git\n",
    "!cd espnet && pip install .\n",
    "!pip install -q espnet_model_zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f2krGs4Q4CM"
   },
   "source": [
    "We also have some other toolkits/packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK4YaZaSRTwn"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "!git clone --depth 1 https://github.com/kan-bayashi/ParallelWaveGAN.git\n",
    "!cd ParallelWaveGAN && pip install .\n",
    "!pip install pysndfile\n",
    "!pip install sacrebleu\n",
    "!pip install mosestokenizer\n",
    "!git clone https://github.com/facebookresearch/SimulEval.git\n",
    "!cd SimulEval && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQsRRIgmP-SA"
   },
   "source": [
    "## Speech Translation\n",
    "\n",
    "Speech translation is a typical task that translate speech in a language into text/speech in another language. In this tutorial, we will show you the some latest models (in ESPnet-ST-v2) in the field of speech translation and demonstrate using them in different scenarios, including\n",
    "\n",
    "- offline speech-to-text translation\n",
    "- simultaneous speech-to-text translation\n",
    "- speech-to-speech translation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-df55eiwtfl"
   },
   "source": [
    "## Overview of the ESPnet-ST-v2\n",
    "\n",
    "ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community.\n",
    "ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) -- each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits.\n",
    "This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. \n",
    "\n",
    "\n",
    "![picture](https://drive.google.com/uc?id=1taR-6Cq4akhhq3oQtgR7Y9mWD0vaEm4z)\n",
    "\n",
    "\n",
    "In general, the toolkit is organizd in a pythonic way to support model training/inference, while we also provide recipes for data preparation, model training, and evaluation.\n",
    "\n",
    "\n",
    "![pitcture](https://drive.google.com/uc?id=1I3w9BAYBhyaf440pBr6VX885f7oJN3mp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHKwsqGUAT1y"
   },
   "source": [
    "## 1. Offline Speech-to-text Translation (ST)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPliTFpknT-O"
   },
   "source": [
    "### 1.1 Model download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzzocJq18hcc"
   },
   "outputs": [],
   "source": [
    "# Download pretrained st model\n",
    "!gdown 1Sn2rAZXVSm1hrCj5OIlq61EgbjKXNGdq\n",
    "!unzip -o st_train_st_ctc_md_conformer_asrinit_v3_noamp_batch50m_ctcsamp0.1_lr1e-3_raw_en_es_bpe_tc4000_sp_valid.acc.ave.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M32GcHjLnWf9"
   },
   "source": [
    "### 1.2 Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ET9KX_NXnX9G"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet2.bin.st_inference import Speech2Text\n",
    "\n",
    "lang=\"es\"\n",
    "fs = 16000\n",
    "\n",
    "speech2text = Speech2Text(\n",
    "    st_model_file=\"/content/exp/st_train_st_ctc_md_conformer_asrinit_v3_noamp_batch50m_ctcsamp0.1_lr1e-3_raw_en_es_bpe_tc4000_sp/valid.acc.ave_10best.pth\",\n",
    "    st_train_config=\"/content/exp/st_train_st_ctc_md_conformer_asrinit_v3_noamp_batch50m_ctcsamp0.1_lr1e-3_raw_en_es_bpe_tc4000_sp/config.yaml\",\n",
    "    beam_size=10,\n",
    "    ctc_weight=0.3,\n",
    "    asr_beam_size=10,\n",
    "    asr_ctc_weight=0.3,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJUnxiV0sCUQ"
   },
   "source": [
    "### 1.3 Translate our example recordings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVrt1clRnocO"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ftshijt/ESPnet_st_egs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "vuWn1Lwmno56"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "bleu = BLEU()\n",
    "\n",
    "egs = pd.read_csv(\"ESPnet_st_egs/st/egs.csv\")\n",
    "for index, row in egs.iterrows():\n",
    "  if row[\"lang\"] == lang or lang == \"multilingual\":\n",
    "    speech, rate = sf.read(\"ESPnet_st_egs/\" + row[\"path\"])\n",
    "    assert fs == int(row[\"sr\"])\n",
    "    text, _, _, _ = speech2text(speech)[0][0]\n",
    "    display(Audio(speech, rate=fs))\n",
    "    librosa.display.waveplot(speech, sr=fs)\n",
    "    plt.show()\n",
    "    print(f\"Reference source text: {row['src_text']}\")\n",
    "    print(f\"Translation results: {text}\")\n",
    "    print(f\"Reference target text: {row['tgt_text']}\")\n",
    "    print(f\"Sentence BLEU Score: {bleu.sentence_score(text, [row['tgt_text']])}\")\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO_-GQHUVnCL"
   },
   "source": [
    "### Task1  (✅ Checkpoint 1 (2 point))\n",
    "\n",
    "We have printout the sentence BLEU score of the model. Can you compute the corpus BLEU with [examples in sacreBLEU](https://github.com/mjpost/sacrebleu#using-sacrebleu-from-python) based on the five utterances in the example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFfIhjZjXx62"
   },
   "outputs": [],
   "source": [
    "# CHECKPOINT1\n",
    "\n",
    "refs = [[\n",
    "    \"Acabo de regresar de una comunidad que tiene el secreto de la supervivencia humana .\",\n",
    "    \"En última instancia , avanzar , creo que tenemos que darle lugar al miedo .\",\n",
    "    \"Cuando recién ingresaba a la universidad tuve mi primer clase de biología .\",\n",
    "    \"Comparte sus experiencias con ellos .\",\n",
    "    \"Cada vez que estén de vacaciones y alguien colapse , puede ser un pariente o alguien enfrente de Uds. , pueden encontrarlo .\",\n",
    "]]\n",
    "\n",
    "# Please fill the translation results to here\n",
    "hyps = [\n",
    "    \n",
    "]\n",
    "\n",
    "# Please compute the corpus bleu score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Peknr-P5VqSI"
   },
   "source": [
    "### 1.4 Translate your own live-recordings\n",
    "1. Record your own voice\n",
    "2. Tralsate your vocie with the ST system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPQvoOlZVq0f"
   },
   "outputs": [],
   "source": [
    "# from https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be\n",
    "\n",
    "from IPython.display import Javascript\n",
    "from google.colab import output\n",
    "from base64 import b64decode\n",
    "\n",
    "RECORD = \"\"\"\n",
    "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "  const reader = new FileReader()\n",
    "  reader.onloadend = e => resolve(e.srcElement.result)\n",
    "  reader.readAsDataURL(blob)\n",
    "})\n",
    "var record = time => new Promise(async resolve => {\n",
    "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "  recorder = new MediaRecorder(stream)\n",
    "  chunks = []\n",
    "  recorder.ondataavailable = e => chunks.push(e.data)\n",
    "  recorder.start()\n",
    "  await sleep(time)\n",
    "  recorder.onstop = async ()=>{\n",
    "    blob = new Blob(chunks)\n",
    "    text = await b2text(blob)\n",
    "    resolve(text)\n",
    "  }\n",
    "  recorder.stop()\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "def record(sec, filename='audio.wav'):\n",
    "  display(Javascript(RECORD))\n",
    "  s = output.eval_js('record(%d)' % (sec * 1000))\n",
    "  b = b64decode(s.split(',')[1])\n",
    "  with open(filename, 'wb+') as f:\n",
    "    f.write(b)\n",
    "\n",
    "audio = 'audio.wav'\n",
    "second = 5\n",
    "print(f\"Speak to your microphone {second} sec...\")\n",
    "record(second, audio)\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "speech, rate = librosa.load(audio, sr=16000)\n",
    "librosa.display.waveplot(speech, sr=rate)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()\n",
    "\n",
    "import pysndfile\n",
    "pysndfile.sndio.write('audio_ds.wav', speech, rate=rate, format='wav', enc='pcm16')\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "display(Audio(speech, rate=rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WsVhzhrVGmc"
   },
   "source": [
    "### Task2  (✅ Checkpoint 2 (1 point))\n",
    "Please follow the same procedure as previous examples and print out the translation results. (You can directly use the `speech2text` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDKEn2m-1wht"
   },
   "outputs": [],
   "source": [
    "# [CHECKPOINT2]\n",
    "# Follow the same procedure as previous examples, print out the translation results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3pm3nJECzlG"
   },
   "source": [
    "## 2. Simultaneous Speech-to-text Translation (SST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DjpPOM0nJyI"
   },
   "outputs": [],
   "source": [
    "# Download retrained sst model\n",
    "!gdown 1ekUeMvmaB3ZhAIY_KAb_we1zhIRFZhtu\n",
    "!unzip -o /content/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp_valid.acc.ave.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2ro0NJkAXKY"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet2.bin.st_inference_streaming import Speech2TextStreaming\n",
    "\n",
    "lang=\"es\"\n",
    "fs = 16000\n",
    "\n",
    "speech2textstreaming = Speech2TextStreaming(\n",
    "    st_model_file=\"/content/exp/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp/valid.acc.ave_10best.pth\",\n",
    "    st_train_config=\"/content/exp/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp/config.yaml\",\n",
    "    penalty=0.4,\n",
    "    blank_penalty=0.5,\n",
    "    beam_size=10,\n",
    "    ctc_weight=0.5,\n",
    "    incremental_decode=True,\n",
    "    time_sync=True,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JdneWnauCXt-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "bleu = BLEU()\n",
    "\n",
    "egs = pd.read_csv(\"ESPnet_st_egs/st/egs.csv\")\n",
    "for index, row in egs.iterrows():\n",
    "  if row[\"lang\"] == lang or lang == \"multilingual\":\n",
    "    speech, rate = sf.read(\"ESPnet_st_egs/\" + row[\"path\"])\n",
    "    assert fs == int(row[\"sr\"])\n",
    "    text = speech2textstreaming(speech)[0][0]\n",
    "    display(Audio(speech, rate=fs))\n",
    "    librosa.display.waveplot(speech, sr=fs)\n",
    "    plt.show()\n",
    "    print(f\"Reference source text: {row['src_text']}\")\n",
    "    print(f\"Translation results: {text}\")\n",
    "    print(f\"Reference target text: {row['tgt_text']}\")\n",
    "    print(f\"Sentence BLEU Score: {bleu.sentence_score(text, [row['tgt_text']])}\")\n",
    "    print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nn9jKCICY0l-"
   },
   "source": [
    "### Question3  (✅ Checkpoint 3 (1 point))\n",
    "How is the performance of the streaming model compared to the offline model? Could you provide some explanation on the performances differences?\n",
    "\n",
    "(For question-based checkpoint: please directly answer it in the text box)\n",
    "\n",
    "[YOUR ANSWER HERE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqUHMeI1ELzW"
   },
   "outputs": [],
   "source": [
    "!simuleval --source /content/ESPnet_st_egs/st/wav.scp --target /content/ESPnet_st_egs/st/ref.detok.trn --agent /content/espnet/egs2/TEMPLATE/st1/pyscripts/utils/simuleval_agent.py --batch_size 1 --ngpu 0 --st_train_config /content/exp/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp/config.yaml --st_model_file exp/st_train_st_ctc_conformer_asrinit_v2_streaming_40block_nohier_18lyr_raw_en_es_bpe_tc4000_sp/valid.acc.ave_10best.pth --disable_repetition_detection false --beam_size 10 --sim_chunk_length 2048 --backend streaming --ctc_weight 0.5 --incremental_decode true --penalty 0.4 --blank_penalty 0.7 --time_sync true --latency-metrics LAAL AL AP DAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXfojYx5Z4-M"
   },
   "source": [
    "### Question4  (✅ Checkpoint 4 (1 point))\n",
    "Despite from BLEU, we have LAACL, AL, AP, DAL for evaluation. AL (average lagging) is one of the most widely used metrics in recent works. Please use one sentence to describe what AL is.\n",
    "\n",
    "(For question-based checkpoint: please directly answer it in the text box)\n",
    "\n",
    "[YOUR ANSWER HERE]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
