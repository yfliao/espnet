{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saK2jMSW8s0T"
   },
   "source": [
    "# ESPnet-S2ST realtime demonstration\n",
    "\n",
    "This notebook provides a demonstration of the realtime end-to-end speech translation using ESPnet-ST-v2.\n",
    "\n",
    "Authors: Jiatong Shi ([@ftshijt](https://github.com/ftshijt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Nzcy8-Mj2Xn"
   },
   "source": [
    "## Setup Envrionments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ut4EsRqWj1FU"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "!git clone --depth 5 -b merge_s2st_st https://github.com/ftshijt/espnet.git\n",
    "!cd espnet && pip install .\n",
    "!git clone --depth 1 https://github.com/kan-bayashi/ParallelWaveGAN.git\n",
    "!cd ParallelWaveGAN && pip install .\n",
    "!pip install -q espnet_model_zoo\n",
    "!pip install pysndfile\n",
    "!pip install sacrebleu\n",
    "!pip install mosestokenizer\n",
    "!git clone https://github.com/facebookresearch/SimulEval.git\n",
    "!cd SimulEval && pip install -e .\n",
    "!pip install typeguard==2.13.3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeD3YwUuCvT6"
   },
   "source": [
    "## Offline Speech-to-speech translation (S2ST)\n",
    "\n",
    "In this demonstration, we show an example model that is trained with discrete-unit-based speech-to-speech translation model. Specifically, the model is trained on Spanish-to-English subset of the [CVSS-C](https://github.com/google-research-datasets/cvss) corpus. The source speech/transcription of CVSS is from commonvoice (read speech); the target transcription is from CoVOST2 (a speech-to-text translation corpus); and the final speech is synthesized by a single-speaker TTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "437IlibpnRHk"
   },
   "source": [
    "### Model download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g3TEZ3vdqE-"
   },
   "outputs": [],
   "source": [
    "# Download pretrained s2st model\n",
    "\n",
    "!gdown 1wNaEebJDDcgi8RZhniKKEMFV15ktcNRE\n",
    "!unzip -o /content/s2st_train_s2st_discrete_unit_raw_fbank_es_en_train.loss.best.zip\n",
    "\n",
    "# Download pretrained unit-based vocoder\n",
    "!gdown 1ezVM3YujTVZSytOeWtD0MsXdqw19AaXa\n",
    "!unzip -o /content/hubert6_500_unit_vocoder.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABEDJ2skiJjA"
   },
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj5s19ObiIzF"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet2.bin.s2st_inference import Speech2Speech\n",
    "\n",
    "# temporary for a buggy checkpoint\n",
    "!cp /content/exp/s2st_stats_raw_es_en/train/src_feats_stats.npz /content/exp/s2st_stats_raw_es_en/train/tgt_feats_stats.npz \n",
    "\n",
    "lang = \"es\"\n",
    "fs = 16000\n",
    "\n",
    "speech2speech = Speech2Speech(\n",
    "    model_file=\"/content/exp/s2st_train_s2st_discrete_unit_raw_fbank_es_en/362epoch.pth\",\n",
    "    train_config=\"/content/exp/s2st_train_s2st_discrete_unit_raw_fbank_es_en/config.yaml\",\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=4,\n",
    "    beam_size=3,\n",
    "    vocoder_file=\"/content/unit_pretrained_vocoder/checkpoint-50000steps.pkl\",\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "def text_normalizer(text):\n",
    "    text = text.upper()\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kh6HFHfjqxFo"
   },
   "outputs": [],
   "source": [
    "# Load ASR models for ASR-BLEU\n",
    "import time\n",
    "import torch\n",
    "import string\n",
    "from espnet_model_zoo.downloader import ModelDownloader\n",
    "from espnet2.bin.asr_inference import Speech2Text as asr\n",
    "\n",
    "tag = \"asapp/e_branchformer_librispeech\"\n",
    "\n",
    "d = ModelDownloader()\n",
    "# It may takes a while to download and build models\n",
    "asr_model = asr(\n",
    "    **d.download_and_unpack(tag),\n",
    "    device=\"cuda\",\n",
    "    minlenratio=0.0,\n",
    "    maxlenratio=0.0,\n",
    "    ctc_weight=0.3,\n",
    "    beam_size=10,\n",
    "    batch_size=0,\n",
    "    nbest=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dugxzzKUiONN"
   },
   "source": [
    "### Translate our example recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-nc7mxRngG4"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ftshijt/ESPnet_st_egs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYt9L7c_iU2u"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "from sacrebleu.metrics import BLEU\n",
    "\n",
    "bleu = BLEU(effective_order=True)\n",
    "\n",
    "egs = pd.read_csv(\"ESPnet_st_egs/s2st/egs.csv\")\n",
    "for index, row in egs.iterrows():\n",
    "  if row[\"lang\"] == lang or lang == \"multilingual\":\n",
    "    speech, rate = sf.read(\"ESPnet_st_egs/\" + row[\"path\"])\n",
    "    assert fs == int(row[\"sr\"])\n",
    "    tensor_speech = torch.tensor(speech, dtype=torch.double).unsqueeze(0).float()\n",
    "    length = tensor_speech.new_full([1], dtype=torch.long, fill_value=tensor_speech.size(1))\n",
    "    output_dict = speech2speech(tensor_speech, length)\n",
    "\n",
    "    output_wav = output_dict[\"wav\"].cpu().numpy()\n",
    "    sf.write(\n",
    "        \"ESPnet_st_egs/{row['path']}.predict.wav\",\n",
    "        output_wav,\n",
    "        fs,\n",
    "        \"PCM_16\",\n",
    "    )\n",
    "    print(f\"Input Speech: ESPnet_st_egs/{row['path']}\")\n",
    "    # let us listen to samples\n",
    "    display(Audio(speech, rate=fs))\n",
    "    librosa.display.waveshow(speech, sr=fs)\n",
    "    plt.show()\n",
    "    print(f\"Reference source text: {text_normalizer(row['src_text'])}\")\n",
    "    print(f\"Reference target text: {text_normalizer(row['tgt_text'])}\")\n",
    "    print(f\"Output speech: ESPnet_st_egs/{row['path']}.predict.wav\")\n",
    "    print(f\"Unit: {output_dict}\")\n",
    "    display(Audio(output_wav, rate=fs))\n",
    "    librosa.display.waveshow(output_wav, sr=fs)\n",
    "    plt.show()\n",
    "\n",
    "    # ASR recognition to the output samples\n",
    "    text, *_ = asr_model(output_wav)[0]\n",
    "    print(text)\n",
    "    print(f\"ASR hypothesis: {text_normalizer(text)}\")\n",
    "    print(f\"ASR BLEU: {bleu.sentence_score(text_normalizer(text), [text_normalizer(row['tgt_text'])])}\")\n",
    "    print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUZSm5F1krC6"
   },
   "source": [
    "### Translate your own pre-recordings\n",
    "1. Upload your own pre-recorded recordings\n",
    "2. Translate your voice with the S2ST system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hNFwFOIKk5aN"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from IPython.display import display, Audio\n",
    "import soundfile\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for file_name in uploaded.keys():\n",
    "  speech, rate = soundfile.read(file_name)\n",
    "  assert rate == fs, \"mismatch in sampling rate\"\n",
    "  tensor_speech = torch.tensor(speech, dtype=torch.double).unsqueeze(0).float()\n",
    "  length = tensor_speech.new_full([1], dtype=torch.long, fill_value=tensor_speech.size(1))\n",
    "  output_dict = speech2speech(tensor_speech, length)\n",
    "\n",
    "  output_wav = output_dict[\"wav\"].cpu().numpy()\n",
    "\n",
    "  print(f\"Input Speech: {file_name}\")\n",
    "  display(Audio(speech, rate=rate))\n",
    "  librosa.display.waveshow(speech, sr=rate)\n",
    "  plt.show()\n",
    "  print(\"*\" * 50)\n",
    "  print(f\"Output speech: predict.wav\")\n",
    "  display(Audio(output_wav, rate=fs))\n",
    "  librosa.display.waveshow(output_wav, sr=fs)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhrpKkpXl10f"
   },
   "source": [
    "### Translate your own live-recordings\n",
    "1. Record your own voice (for people cannot speak Spanish, you may find some web sources and play that with your phone to simulate the real-time recording).\n",
    "2. Tralsate your vocie with the S2ST system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIT-Q1KfmCmg"
   },
   "outputs": [],
   "source": [
    "# from https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be\n",
    "\n",
    "from IPython.display import Javascript\n",
    "from google.colab import output\n",
    "from base64 import b64decode\n",
    "\n",
    "RECORD = \"\"\"\n",
    "const sleep = time => new Promise(resolve => setTimeout(resolve, time))\n",
    "const b2text = blob => new Promise(resolve => {\n",
    "  const reader = new FileReader()\n",
    "  reader.onloadend = e => resolve(e.srcElement.result)\n",
    "  reader.readAsDataURL(blob)\n",
    "})\n",
    "var record = time => new Promise(async resolve => {\n",
    "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
    "  recorder = new MediaRecorder(stream)\n",
    "  chunks = []\n",
    "  recorder.ondataavailable = e => chunks.push(e.data)\n",
    "  recorder.start()\n",
    "  await sleep(time)\n",
    "  recorder.onstop = async ()=>{\n",
    "    blob = new Blob(chunks)\n",
    "    text = await b2text(blob)\n",
    "    resolve(text)\n",
    "  }\n",
    "  recorder.stop()\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "def record(sec, filename='audio.wav'):\n",
    "  display(Javascript(RECORD))\n",
    "  s = output.eval_js('record(%d)' % (sec * 1000))\n",
    "  b = b64decode(s.split(',')[1])\n",
    "  with open(filename, 'wb+') as f:\n",
    "    f.write(b)\n",
    "\n",
    "audio = 'audio.wav'\n",
    "second = 5\n",
    "print(f\"Speak to your microphone {second} sec...\")\n",
    "record(second, audio)\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "speech, rate = librosa.load(audio, sr=16000)\n",
    "librosa.display.waveshow(speech, sr=rate)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.show()\n",
    "\n",
    "import pysndfile\n",
    "pysndfile.sndio.write('audio_ds.wav', speech, rate=rate, format='wav', enc='pcm16')\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "display(Audio(speech, rate=rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvxGwRUAmVXF"
   },
   "outputs": [],
   "source": [
    "tensor_speech = torch.tensor(speech, dtype=torch.double).unsqueeze(0).float()\n",
    "length = tensor_speech.new_full([1], dtype=torch.long, fill_value=tensor_speech.size(1))\n",
    "output_dict = speech2speech(tensor_speech, length)\n",
    "\n",
    "output_wav = output_dict[\"wav\"].cpu().numpy()\n",
    "\n",
    "print(f\"Output speech: predict.wav\")\n",
    "display(Audio(output_wav, rate=fs))\n",
    "librosa.display.waveshow(output_wav, sr=fs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
